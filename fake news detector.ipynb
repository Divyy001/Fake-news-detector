{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d067af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2022.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pycaret\n",
      "  Downloading pycaret-3.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting category-encoders>=2.4.0 (from pycaret)\n",
      "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting cloudpickle (from pycaret)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting deprecation>=2.1.0 (from pycaret)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: imbalanced-learn>=0.8.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (0.9.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.12.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (5.0.0)\n",
      "Requirement already satisfied: ipython>=5.5.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (8.4.0)\n",
      "Requirement already satisfied: ipywidgets>=7.6.5 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (8.0.6)\n",
      "Requirement already satisfied: jinja2>=1.2 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (3.1.2)\n",
      "Collecting joblib>=1.2.0 (from pycaret)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting kaleido>=0.2.1 (from pycaret)\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-win_amd64.whl (65.9 MB)\n",
      "     ---------------------------------------- 65.9/65.9 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting lightgbm>=3.0.0 (from pycaret)\n",
      "  Downloading lightgbm-4.2.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: markupsafe>=2.0.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (2.1.1)\n",
      "Requirement already satisfied: matplotlib<=3.6,>=3.3.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (3.5.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (5.4.0)\n",
      "Requirement already satisfied: numba>=0.55.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (0.56.4)\n",
      "Requirement already satisfied: numpy<1.27,>=1.21 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (1.24.3)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.3.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (1.4.3)\n",
      "Collecting plotly-resampler>=0.8.3.1 (from pycaret)\n",
      "  Downloading plotly_resampler-0.9.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: plotly>=5.0.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (5.18.0)\n",
      "Collecting pmdarima!=1.8.1,<3.0.0,>=1.8.0 (from pycaret)\n",
      "  Downloading pmdarima-2.0.4-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: psutil>=5.9.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (5.9.1)\n",
      "Collecting pyod>=1.0.8 (from pycaret)\n",
      "  Downloading pyod-1.1.2.tar.gz (160 kB)\n",
      "     -------------------------------------- 160.5/160.5 kB 3.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.27.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (2.28.1)\n",
      "Collecting schemdraw==0.15 (from pycaret)\n",
      "  Downloading schemdraw-0.15-py3-none-any.whl (106 kB)\n",
      "     -------------------------------------- 106.8/106.8 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn<1.3.0,>=1.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (1.1.1)\n",
      "Collecting scikit-plot>=0.3.7 (from pycaret)\n",
      "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
      "Collecting scipy~=1.10.1 (from pycaret)\n",
      "  Downloading scipy-1.10.1-cp310-cp310-win_amd64.whl (42.5 MB)\n",
      "     ---------------------------------------- 42.5/42.5 MB 1.5 MB/s eta 0:00:00\n",
      "Collecting sktime!=0.17.1,!=0.17.2,!=0.18.0,<0.22.0,>=0.16.1 (from pycaret)\n",
      "  Downloading sktime-0.21.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting statsmodels>=0.12.1 (from pycaret)\n",
      "  Downloading statsmodels-0.14.1-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting tbats>=1.1.3 (from pycaret)\n",
      "  Downloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
      "     ---------------------------------------- 44.0/44.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.62.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pycaret) (4.64.0)\n",
      "Collecting xxhash (from pycaret)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yellowbrick>=1.4 (from pycaret)\n",
      "  Downloading yellowbrick-1.5-py3-none-any.whl (282 kB)\n",
      "     -------------------------------------- 282.6/282.6 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting patsy>=0.5.1 (from category-encoders>=2.4.0->pycaret)\n",
      "  Downloading patsy-0.5.5-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from deprecation>=2.1.0->pycaret) (21.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from imbalanced-learn>=0.8.1->pycaret) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from importlib-metadata>=4.12.0->pycaret) (3.10.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (3.0.30)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\program files\\python310\\lib\\site-packages (from ipython>=5.5.0->pycaret) (58.1.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (0.3.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (5.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=5.5.0->pycaret) (0.4.5)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets>=7.6.5->pycaret) (6.15.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets>=7.6.5->pycaret) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets>=7.6.5->pycaret) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (1.4.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (2.8.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from nbformat>=4.2.0->pycaret) (4.6.1)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from nbformat>=4.2.0->pycaret) (4.10.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from nbformat>=4.2.0->pycaret) (2.15.3)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from numba>=0.55.0->pycaret) (0.39.1)\n",
      "Collecting numpy<1.27,>=1.21 (from pycaret)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-win_amd64.whl (14.6 MB)\n",
      "     ---------------------------------------- 14.6/14.6 MB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pandas<2.0.0,>=1.3.0->pycaret) (2022.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from plotly>=5.0.0->pycaret) (8.2.2)\n",
      "Collecting dash<3.0.0,>=2.11.0 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading dash-2.14.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting orjson<4.0.0,>=3.8.0 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading orjson-3.9.10-cp310-none-win_amd64.whl.metadata (50 kB)\n",
      "     -------------------------------------- 50.5/50.5 kB 428.0 kB/s eta 0:00:00\n",
      "Collecting trace-updater>=0.0.8 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading trace_updater-0.0.9.1-py3-none-any.whl (185 kB)\n",
      "     -------------------------------------- 185.2/185.2 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting tsdownsample==0.1.2 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading tsdownsample-0.1.2-cp310-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting Cython!=0.29.18,!=0.29.31,>=0.29 (from pmdarima!=1.8.1,<3.0.0,>=1.8.0->pycaret)\n",
      "  Downloading Cython-3.0.7-cp310-cp310-win_amd64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pmdarima!=1.8.1,<3.0.0,>=1.8.0->pycaret) (1.26.9)\n",
      "Requirement already satisfied: six in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from pyod>=1.0.8->pycaret) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.27.1->pycaret) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.27.1->pycaret) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.27.1->pycaret) (2022.6.15)\n",
      "Collecting deprecated>=1.2.13 (from sktime!=0.17.1,!=0.17.2,!=0.18.0,<0.22.0,>=0.16.1->pycaret)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-base<0.6.0 (from sktime!=0.17.1,!=0.17.2,!=0.18.0,<0.22.0,>=0.16.1->pycaret)\n",
      "  Downloading scikit_base-0.5.2-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting Flask<3.1,>=1.0.4 (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading flask-3.0.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: Werkzeug<3.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (2.1.2)\n",
      "Collecting dash-html-components==2.0.0 (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
      "Collecting dash-core-components==2.0.0 (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
      "Collecting dash-table==5.0.0 (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (4.3.0)\n",
      "Collecting retrying (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Collecting ansi2html (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading ansi2html-1.9.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (1.5.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from deprecated>=1.2.13->sktime!=0.17.1,!=0.17.2,!=0.18.0,<0.22.0,>=0.16.1->pycaret) (1.14.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (1.6.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (7.3.4)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=5.5.0->pycaret) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (0.18.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->pycaret) (0.2.5)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core->nbformat>=4.2.0->pycaret) (304)\n",
      "Requirement already satisfied: executing in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=5.5.0->pycaret) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=5.5.0->pycaret) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=5.5.0->pycaret) (0.2.2)\n",
      "Collecting Werkzeug<3.1 (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\program files\\python310\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (8.1.3)\n",
      "Collecting blinker>=1.6.2 (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\divyy\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (0.4)\n",
      "Downloading pycaret-3.2.0-py3-none-any.whl (484 kB)\n",
      "   ---------------------------------------- 484.7/484.7 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 81.9/81.9 kB 918.2 kB/s eta 0:00:00\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Downloading lightgbm-4.2.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 1.3/1.3 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading plotly_resampler-0.9.1-py3-none-any.whl (73 kB)\n",
      "   ---------------------------------------- 73.4/73.4 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading pmdarima-2.0.4-cp310-cp310-win_amd64.whl (613 kB)\n",
      "   ---------------------------------------- 613.3/613.3 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading sktime-0.21.1-py3-none-any.whl (17.1 MB)\n",
      "   ---------------------------------------- 17.1/17.1 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading statsmodels-0.14.1-cp310-cp310-win_amd64.whl (9.8 MB)\n",
      "   ---------------------------------------- 9.8/9.8 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Downloading Cython-3.0.7-cp310-cp310-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 2.8/2.8 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading dash-2.14.2-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 10.2/10.2 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading orjson-3.9.10-cp310-none-win_amd64.whl (135 kB)\n",
      "   ---------------------------------------- 135.0/135.0 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading patsy-0.5.5-py2.py3-none-any.whl (234 kB)\n",
      "   ---------------------------------------- 234.1/234.1 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading scikit_base-0.5.2-py3-none-any.whl (118 kB)\n",
      "   ---------------------------------------- 119.0/119.0 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "   ---------------------------------------- 99.7/99.7 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "   ---------------------------------------- 226.7/226.7 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading ansi2html-1.9.1-py3-none-any.whl (17 kB)\n",
      "Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Building wheels for collected packages: pyod\n",
      "  Building wheel for pyod (setup.py): started\n",
      "  Building wheel for pyod (setup.py): finished with status 'done'\n",
      "  Created wheel for pyod: filename=pyod-1.1.2-py3-none-any.whl size=190314 sha256=c7252f432845d4910559ef1e9f8a0e946ca7af24f8a7e78eb5a0289dd74c495b\n",
      "  Stored in directory: c:\\users\\divyy\\appdata\\local\\pip\\cache\\wheels\\81\\1b\\61\\aa85b78c3c0c8871f4231e3f4a03bb23cecb7db829498380ee\n",
      "Successfully built pyod\n",
      "Installing collected packages: trace-updater, kaleido, dash-table, dash-html-components, dash-core-components, xxhash, Werkzeug, scikit-base, schemdraw, retrying, orjson, numpy, joblib, itsdangerous, deprecated, Cython, cloudpickle, blinker, ansi2html, tsdownsample, scipy, patsy, Flask, deprecation, statsmodels, lightgbm, dash, yellowbrick, sktime, scikit-plot, pyod, pmdarima, plotly-resampler, category-encoders, tbats, pycaret\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.1.2\n",
      "    Uninstalling Werkzeug-2.1.2:\n",
      "      Successfully uninstalled Werkzeug-2.1.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.5\n",
      "    Uninstalling blinker-1.5:\n",
      "      Successfully uninstalled blinker-1.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "Successfully installed Cython-3.0.7 Flask-3.0.0 Werkzeug-3.0.1 ansi2html-1.9.1 blinker-1.7.0 category-encoders-2.6.3 cloudpickle-3.0.0 dash-2.14.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 deprecated-1.2.14 deprecation-2.1.0 itsdangerous-2.1.2 joblib-1.3.2 kaleido-0.2.1 lightgbm-4.2.0 numpy-1.23.5 orjson-3.9.10 patsy-0.5.5 plotly-resampler-0.9.1 pmdarima-2.0.4 pycaret-3.2.0 pyod-1.1.2 retrying-1.3.4 schemdraw-0.15 scikit-base-0.5.2 scikit-plot-0.3.7 scipy-1.10.1 sktime-0.21.1 statsmodels-0.14.1 tbats-1.1.3 trace-updater-0.0.9.1 tsdownsample-0.1.2 xxhash-3.4.1 yellowbrick-1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "rembg 2.0.25 requires aiohttp==3.8.1, but you have aiohttp 3.8.4 which is incompatible.\n",
      "rembg 2.0.25 requires numpy==1.21.6, but you have numpy 1.23.5 which is incompatible.\n",
      "rembg 2.0.25 requires scipy==1.7.3, but you have scipy 1.10.1 which is incompatible.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Installing specific libraries\n",
    "! pip install transformers\n",
    "! pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862ef5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycaret\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50cf9c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump Horrifically Declares Getting An Aborti...</td>\n",
       "      <td>It s alarming that so many people are lining u...</td>\n",
       "      <td>News</td>\n",
       "      <td>March 30, 2016</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(VIDEO) TERROR THREAT ALERT: AMERICA CELEBRATE...</td>\n",
       "      <td>Be safe and enjoy the 4th! Homeland Security a...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Jul 4, 2015</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump lawyers make final Supreme Court pitch o...</td>\n",
       "      <td>WASHINGTON (Reuters) - The Trump administratio...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 21, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Key U.S. lawmaker suggests openness to encrypt...</td>\n",
       "      <td>WASHINGTON (Reuters) - A senior U.S. lawmaker ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>February 18, 2016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HILLARY CLINTON CRASHING IN POLLS: Moves To Ob...</td>\n",
       "      <td>So, the working people of America are basicall...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Aug 10, 2015</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Trump Horrifically Declares Getting An Aborti...   \n",
       "1  (VIDEO) TERROR THREAT ALERT: AMERICA CELEBRATE...   \n",
       "2  Trump lawyers make final Supreme Court pitch o...   \n",
       "3  Key U.S. lawmaker suggests openness to encrypt...   \n",
       "4  HILLARY CLINTON CRASHING IN POLLS: Moves To Ob...   \n",
       "\n",
       "                                                text          subject  \\\n",
       "0  It s alarming that so many people are lining u...             News   \n",
       "1  Be safe and enjoy the 4th! Homeland Security a...  Government News   \n",
       "2  WASHINGTON (Reuters) - The Trump administratio...     politicsNews   \n",
       "3  WASHINGTON (Reuters) - A senior U.S. lawmaker ...     politicsNews   \n",
       "4  So, the working people of America are basicall...  Government News   \n",
       "\n",
       "                 date Target  \n",
       "0      March 30, 2016   Fake  \n",
       "1         Jul 4, 2015   Fake  \n",
       "2      June 21, 2017    True  \n",
       "3  February 18, 2016    True  \n",
       "4        Aug 10, 2015   Fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "true_data = pd.read_csv('a1_True.csv')\n",
    "fake_data = pd.read_csv('a2_Fake.csv')\n",
    "\n",
    "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
    "true_data['Target'] = ['True']*len(true_data)\n",
    "fake_data['Target'] = ['Fake']*len(fake_data)\n",
    "\n",
    "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
    "data = true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index'])\n",
    "\n",
    "# See how the data looks like\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb4e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column is made of string values True/Fake, let's change it to numbers 0/1 (Fake=1) \n",
    "data['label'] = pd.get_dummies(data.Target)['Fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb243c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump Horrifically Declares Getting An Aborti...</td>\n",
       "      <td>It s alarming that so many people are lining u...</td>\n",
       "      <td>News</td>\n",
       "      <td>March 30, 2016</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(VIDEO) TERROR THREAT ALERT: AMERICA CELEBRATE...</td>\n",
       "      <td>Be safe and enjoy the 4th! Homeland Security a...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Jul 4, 2015</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump lawyers make final Supreme Court pitch o...</td>\n",
       "      <td>WASHINGTON (Reuters) - The Trump administratio...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 21, 2017</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Key U.S. lawmaker suggests openness to encrypt...</td>\n",
       "      <td>WASHINGTON (Reuters) - A senior U.S. lawmaker ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>February 18, 2016</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HILLARY CLINTON CRASHING IN POLLS: Moves To Ob...</td>\n",
       "      <td>So, the working people of America are basicall...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Aug 10, 2015</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Trump Horrifically Declares Getting An Aborti...   \n",
       "1  (VIDEO) TERROR THREAT ALERT: AMERICA CELEBRATE...   \n",
       "2  Trump lawyers make final Supreme Court pitch o...   \n",
       "3  Key U.S. lawmaker suggests openness to encrypt...   \n",
       "4  HILLARY CLINTON CRASHING IN POLLS: Moves To Ob...   \n",
       "\n",
       "                                                text          subject  \\\n",
       "0  It s alarming that so many people are lining u...             News   \n",
       "1  Be safe and enjoy the 4th! Homeland Security a...  Government News   \n",
       "2  WASHINGTON (Reuters) - The Trump administratio...     politicsNews   \n",
       "3  WASHINGTON (Reuters) - A senior U.S. lawmaker ...     politicsNews   \n",
       "4  So, the working people of America are basicall...  Government News   \n",
       "\n",
       "                 date Target  label  \n",
       "0      March 30, 2016   Fake      1  \n",
       "1         Jul 4, 2015   Fake      1  \n",
       "2      June 21, 2017    True      0  \n",
       "3  February 18, 2016    True      0  \n",
       "4        Aug 10, 2015   Fake      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1073a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.patches.Wedge at 0x23b83592230>,\n",
       "  <matplotlib.patches.Wedge at 0x23b83592bc0>],\n",
       " [Text(-1.1968727067385088, -0.0865778485782335, 'Fake'),\n",
       "  Text(1.1968726986325005, 0.08657796063754254, 'True')],\n",
       " [Text(-0.6981757455974634, -0.05050374500396954, '52.3%'),\n",
       "  Text(0.6981757408689586, 0.05050381037189981, '47.7%')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking if our data is well balanced\n",
    "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
    "plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],startangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a187d2",
   "metadata": {},
   "source": [
    "### Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bfe805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation-Test set split into 70:15:15 ratio\n",
    "# Train-Temp split\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(data['title'], data['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=data['Target'])\n",
    "# Validation-Test split\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf3c83",
   "metadata": {},
   "source": [
    "### Bert Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e51982a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.09102678298950195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5044fd7e3d154e82ac16f4d58b74712d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07129549980163574,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 440473133,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac49a8927d145319d04f0d4f785499f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07164716720581055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 28,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3486034f164404d9aaf8e07a9d07735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04425215721130371,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7171b8411c4041a8900ec53215c54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04227185249328613,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 466062,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef8508ede5b46b280b3a91c1f0ae10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Loading Pretrained BERT Model\n",
    "\n",
    "# Load BERT model and tokenizer via HuggingFace Transformers\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3511397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of texts')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Prepare Input Data\n",
    "\n",
    "# Plot histogram of the number of words in train data 'title'\n",
    "seq_len = [len(title.split()) for title in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 40,color='firebrick')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0abd109e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 3857, 8275, 2739, 2944, 1012, 102], [101, 2478, 14324, 1012, 102, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# BERT Tokeizer Functionality\n",
    "sample_data = [\"Build fake news model.\", \n",
    "               \"Using bert.\"]                                         # sample data\n",
    "tokenized_sample_data = tokenizer.batch_encode_plus(sample_data,\n",
    "                                                    padding=True)     # encode text\n",
    "print(tokenized_sample_data)\n",
    "\n",
    "# Ref: https://huggingface.co/docs/transformers/preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8a7226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of titles above have word length under 15. So, we set max title length as 15\n",
    "MAX_LENGHT = 15\n",
    "# Tokenize and encode sequences in the train set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bebd7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4291bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader structure definition\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32                                               #define a batch size\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)    # wrap tensors\n",
    "train_sampler = RandomSampler(train_data)                     # sampler for sampling the data during training\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "                                                              # dataLoader for train set\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)            # wrap tensors\n",
    "val_sampler = SequentialSampler(val_data)                     # sampler for sampling the data during training\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
    "                                                              # dataLoader for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ebe389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze Layers\n",
    "\n",
    "# Freezing the parameters and defining trainable BERT structure\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False    # false here means gradient need not be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8250d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining model Architecture\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):  \n",
    "      super(BERT_Arch, self).__init__()\n",
    "      self.bert = bert   \n",
    "      self.dropout = nn.Dropout(0.1)            # dropout layer\n",
    "      self.relu =  nn.ReLU()                    # relu activation function\n",
    "      self.fc1 = nn.Linear(768,512)             # dense layer 1\n",
    "      self.fc2 = nn.Linear(512,2)               # dense layer 2 (Output layer)\n",
    "      self.softmax = nn.LogSoftmax(dim=1)       # softmax activation function\n",
    "    def forward(self, sent_id, mask):           # define the forward pass  \n",
    "      cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
    "                                                # pass the inputs to the model\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)                           # output layer\n",
    "      x = self.softmax(x)                       # apply softmax activation\n",
    "      return x\n",
    "\n",
    "model = BERT_Arch(bert)\n",
    "# Defining the hyperparameters (optimizer, weights of the classes and the epochs)\n",
    "# Define the optimizer\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)          # learning rate\n",
    "# Define the loss function\n",
    "cross_entropy  = nn.NLLLoss() \n",
    "# Number of training epochs\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2bc0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining Train and Evaluate Function\n",
    "\n",
    "# Defining training and evaluation functions\n",
    "def train():  \n",
    "  model.train()\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  for step,batch in enumerate(train_dataloader):                # iterate over batches\n",
    "    if step % 50 == 0 and not step == 0:                        # progress update after every 50 batches.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "    batch = [r for r in batch]                                  # push the batch to gpu\n",
    "    sent_id, mask, labels = batch \n",
    "    model.zero_grad()                                           # clear previously calculated gradients\n",
    "    preds = model(sent_id, mask)                                # get model predictions for current batch\n",
    "    loss = cross_entropy(preds, labels)                         # compute loss between actual & predicted values\n",
    "    total_loss = total_loss + loss.item()                       # add on to the total loss\n",
    "    loss.backward()                                             # backward pass to calculate the gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)     # clip gradients to 1.0. It helps in preventing exploding gradient problem\n",
    "    optimizer.step()                                            # update parameters\n",
    "    preds=preds.detach().cpu().numpy()                          # model predictions are stored on GPU. So, push it to CPU\n",
    "\n",
    "  avg_loss = total_loss / len(train_dataloader)                 # compute training loss of the epoch  \n",
    "                                                                # reshape predictions in form of (# samples, # classes)\n",
    "  return avg_loss                                 # returns the loss and predictions\n",
    "\n",
    "def evaluate():  \n",
    "  print(\"\\nEvaluating...\")  \n",
    "  model.eval()                                    # Deactivate dropout layers\n",
    "  total_loss, total_accuracy = 0, 0  \n",
    "  for step,batch in enumerate(val_dataloader):    # Iterate over batches  \n",
    "    if step % 50 == 0 and not step == 0:          # Progress update every 50 batches.     \n",
    "                                                  # Calculate elapsed time in minutes.\n",
    "                                                  # Elapsed = format_time(time.time() - t0)\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "                                                  # Report progress\n",
    "    batch = [t for t in batch]                    # Push the batch to GPU\n",
    "    sent_id, mask, labels = batch\n",
    "    with torch.no_grad():                         # Deactivate autograd\n",
    "      preds = model(sent_id, mask)                # Model predictions\n",
    "      loss = cross_entropy(preds,labels)          # Compute the validation loss between actual and predicted values\n",
    "      total_loss = total_loss + loss.item()\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "  avg_loss = total_loss / len(val_dataloader)         # compute the validation loss of the epoch\n",
    "  return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1856c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "  Batch    50  of    983.\n",
      "  Batch   100  of    983.\n",
      "  Batch   150  of    983.\n",
      "  Batch   200  of    983.\n",
      "  Batch   250  of    983.\n",
      "  Batch   300  of    983.\n",
      "  Batch   350  of    983.\n",
      "  Batch   400  of    983.\n",
      "  Batch   450  of    983.\n",
      "  Batch   500  of    983.\n",
      "  Batch   550  of    983.\n",
      "  Batch   600  of    983.\n",
      "  Batch   650  of    983.\n",
      "  Batch   700  of    983.\n",
      "  Batch   750  of    983.\n",
      "  Batch   800  of    983.\n",
      "  Batch   850  of    983.\n",
      "  Batch   900  of    983.\n",
      "  Batch   950  of    983.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    211.\n",
      "  Batch   100  of    211.\n",
      "  Batch   150  of    211.\n",
      "  Batch   200  of    211.\n",
      "\n",
      "Training Loss: 0.560\n",
      "Validation Loss: 0.493\n",
      "\n",
      " Epoch 2 / 2\n",
      "  Batch    50  of    983.\n",
      "  Batch   100  of    983.\n",
      "  Batch   150  of    983.\n",
      "  Batch   200  of    983.\n",
      "  Batch   250  of    983.\n",
      "  Batch   300  of    983.\n",
      "  Batch   350  of    983.\n",
      "  Batch   400  of    983.\n",
      "  Batch   450  of    983.\n",
      "  Batch   500  of    983.\n",
      "  Batch   550  of    983.\n",
      "  Batch   600  of    983.\n",
      "  Batch   650  of    983.\n",
      "  Batch   700  of    983.\n",
      "  Batch   750  of    983.\n",
      "  Batch   800  of    983.\n",
      "  Batch   850  of    983.\n",
      "  Batch   900  of    983.\n",
      "  Batch   950  of    983.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    211.\n",
      "  Batch   100  of    211.\n",
      "  Batch   150  of    211.\n",
      "  Batch   200  of    211.\n",
      "\n",
      "Training Loss: 0.470\n",
      "Validation Loss: 0.410\n"
     ]
    }
   ],
   "source": [
    "## Model Training\n",
    "\n",
    "# Train and predict\n",
    "best_valid_loss = float('inf')\n",
    "train_losses=[]                   # empty lists to store training and validation loss of each epoch\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(epochs):     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))     \n",
    "    train_loss = train()                       # train model\n",
    "    valid_loss = evaluate()                    # evaluate model\n",
    "    if valid_loss < best_valid_loss:              # save the best model\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'c2_new_model_weights.pt')\n",
    "    train_losses.append(train_loss)               # append training and validation loss\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90f8f324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model Performance\n",
    "\n",
    "# load weights of best model\n",
    "path = 'c2_new_model_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7940e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84      3212\n",
      "           1       0.91      0.75      0.82      3523\n",
      "\n",
      "    accuracy                           0.83      6735\n",
      "   macro avg       0.84      0.84      0.83      6735\n",
      "weighted avg       0.84      0.83      0.83      6735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  preds = model(test_seq, test_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6013ecc8",
   "metadata": {},
   "source": [
    "## Fake News Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4e79a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading weights, if imported otherwise\n",
    "\n",
    "# path = 'c2_new_model_weights.pt'\n",
    "# model.load_state_dict(torch.load(path))\n",
    "\n",
    "# testing on unseen data\n",
    "unseen_news_text = [\"Donald Trump Sends Out Embarrassing New Years Eve Message; This is Disturbing\",     # Fake\n",
    "                    \"WATCH: George W. Bush Calls Out Trump For Supporting White Supremacy\",               # Fake\n",
    "                    \"U.S. lawmakers question businessman at 2016 Trump Tower meeting: sources\",           # True\n",
    "                    \"Trump administration issues new rules on U.S. visa waivers\"                          # True\n",
    "                    ]\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "MAX_LENGHT = 15\n",
    "tokens_unseen = tokenizer.batch_encode_plus(\n",
    "    unseen_news_text,\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "unseen_seq = torch.tensor(tokens_unseen['input_ids'])\n",
    "unseen_mask = torch.tensor(tokens_unseen['attention_mask'])\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = model(unseen_seq, unseen_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0270372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61739d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8420b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22231522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d47ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873d836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da6412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cbda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
